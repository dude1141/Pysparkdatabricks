{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568d3a8c-fcb2-4226-907e-428043b5b15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>Alice</td><td>21</td></tr><tr><td>Bob</td><td>23</td></tr><tr><td>Eat</td><td>43</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         "21"
        ],
        [
         "Bob",
         "23"
        ],
        [
         "Eat",
         "43"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataframes\n",
    "\n",
    "data= [('Alice','21'),('Bob','23'),('Eat','43')]\n",
    "schema=[\"Name\",\"Age\"]\n",
    "\n",
    "v1=spark.createDataFrame(data,schema=schema)\n",
    "display(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1723a06c-5955-43c1-b2b6-bab11de02ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>Alice</td><td>21</td></tr><tr><td>Bob</td><td>23</td></tr><tr><td>Eat</td><td>43</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         21
        ],
        [
         "Bob",
         23
        ],
        [
         "Eat",
         43
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType,StructField,StructType, IntegerType\n",
    "\n",
    "schema=StructType([StructField(\"Name\",StringType(),True),StructField(\"Age\",IntegerType(),True)])\n",
    "\n",
    "#or\n",
    "\n",
    "data= [('Alice',21),('Bob',23),('Eat',43)]\n",
    "\n",
    "df_csv=spark.createDataFrame(data,schema=schema)\n",
    "\n",
    "display(df_csv)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26a1dc6d-b8e4-4178-9e6c-ff1beb651958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be313b6-5d61-4f04-8d38-40514308f3a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>Alice</td><td>21</td></tr><tr><td>Bob</td><td>23</td></tr><tr><td>Eat</td><td>43</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         "21"
        ],
        [
         "Bob",
         "23"
        ],
        [
         "Eat",
         "43"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    ('Alice', '21'),\n",
    "    ('Bob', '23'),\n",
    "    ('Eat', '43')\n",
    "]\n",
    "schema = [\"Name\", \"Age\"]\n",
    "\n",
    "v1 = spark.createDataFrame(\n",
    "    data,\n",
    "    schema=schema\n",
    ")\n",
    "v1.createOrReplaceGlobalTempView(\"globtempview\")\n",
    "\n",
    "df2 = spark.sql(\n",
    "    \"SELECT * FROM global_temp.globtempview\"\n",
    ")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb64de6-7315-4891-87cf-27522300d712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderId</th><th>Product</th><th>Quantity</th><th>Price</th><th>Date</th></tr></thead><tbody><tr><td>8</td><td>Keyboard</td><td>10</td><td>75.0</td><td>2024-07-08 00:00:00</td></tr><tr><td>10</td><td> Mouse</td><td>7</td><td>null</td><td>2024-07-10 00:00:00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "8",
         "Keyboard",
         10,
         75.0,
         "2024-07-08 00:00:00"
        ],
        [
         "10",
         " Mouse",
         7,
         null,
         "2024-07-10 00:00:00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"OrderId\", StringType(),True),\n",
    "  StructField(\"Product\", StringType(),True),\n",
    "  StructField(\"Quantity\", IntegerType(),True),\n",
    "  StructField(\"Price\", FloatType(),True),\n",
    "  StructField(\"Date\", StringType(),True)\n",
    "]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"Laptop\", 2, 1500.0, \"2024-07-01 00:00:00\"),\n",
    "    (\"2\", \"Mouse\", 5, 25.0, \"2024-07-02 00:00:00\"),\n",
    "    (\"3\", \"Monitor\", 3, 300.0, \"2024-07-03 00:00:00\"),\n",
    "    (\"4\", \"Keyboard\", 4, 75.0, \"2024-07-04 00:00:00\"),\n",
    "    (\"5\", \"Laptop\", 1, 1500.0, \"2024-07-05 00:00:00\"),\n",
    "    (\"6\", \"Mouse \", 2, 25.0, None),\n",
    "    (\"7\", \" Monitor \", 5, 300.0, \"2024-07-07 00:00:00\"),\n",
    "    (\"8\", \"Keyboard\", 10, 75.0, \"2024-07-08 00:00:00\"),\n",
    "    (\"9\", \"Laptop\", 3, 1500.0, \"2024-07-09 00:00:00\"),\n",
    "    (\"10\", \" Mouse\", 7, None, \"2024-07-10 00:00:00\")\n",
    "]\n",
    "\n",
    "#Dataframe Creation\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "#show(): Display the DataFrame\n",
    "#df.show()\n",
    "\n",
    "#printSchema(): Print the schema of the DataFrame\n",
    "#df.printSchema()\n",
    "\n",
    "df.columns\n",
    "\n",
    "#df.select(\"Quantity\").show()\n",
    "\n",
    "#display(df.select(df.columns[:3]))\n",
    "#display(df.select(df.columns[1:3]))\n",
    "display(df.filter(df.Quantity>5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5052a145-406d-4d10-b5a9-a334b0ebbdf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6806929837598819>, line 5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiscount\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28mround\u001B[39m, col, expr\n",
       "\u001B[0;32m----> 5\u001B[0m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiscount\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mround\u001B[39m(expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice - discount\u001B[39m\u001B[38;5;124m\"\u001B[39m)), \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3853\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   3809\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3810\u001B[0m \n",
       "\u001B[1;32m   3811\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3851\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3852\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3853\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jcols(\u001B[38;5;241m*\u001B[39mcols))\n",
       "\u001B[1;32m   3854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3349\u001B[0m, in \u001B[0;36mDataFrame._jcols\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   3347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cols) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(cols[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[1;32m   3348\u001B[0m     cols \u001B[38;5;241m=\u001B[39m cols[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[0;32m-> 3349\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq(cols, _to_java_column)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3336\u001B[0m, in \u001B[0;36mDataFrame._jseq\u001B[0;34m(self, cols, converter)\u001B[0m\n",
       "\u001B[1;32m   3330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_jseq\u001B[39m(\n",
       "\u001B[1;32m   3331\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m   3332\u001B[0m     cols: Sequence,\n",
       "\u001B[1;32m   3333\u001B[0m     converter: Optional[Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   3334\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _to_seq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39m_sc, cols, converter)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m_to_seq\u001B[0;34m(sc, cols, converter)\u001B[0m\n",
       "\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m \n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n",
       "\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n",
       "\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n",
       "\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m \n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n",
       "\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n",
       "\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n",
       "\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:69\u001B[0m, in \u001B[0;36m_to_java_column\u001B[0;34m(col)\u001B[0m\n",
       "\u001B[1;32m     67\u001B[0m     jcol \u001B[38;5;241m=\u001B[39m _create_column_from_name(col)\n",
       "\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 69\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m     70\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     71\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m     72\u001B[0m     )\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m jcol\n",
       "\n",
       "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkTypeError",
        "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
       },
       "metadata": {
        "errorSummary": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOT_COLUMN_OR_STR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkTypeError\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-6806929837598819>, line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiscount\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28mround\u001B[39m, col, expr\n\u001B[0;32m----> 5\u001B[0m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiscount\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mround\u001B[39m(expr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprice - discount\u001B[39m\u001B[38;5;124m\"\u001B[39m)), \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3853\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   3808\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   3809\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3810\u001B[0m \n\u001B[1;32m   3811\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3851\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3852\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3853\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jcols(\u001B[38;5;241m*\u001B[39mcols))\n\u001B[1;32m   3854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3349\u001B[0m, in \u001B[0;36mDataFrame._jcols\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   3347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cols) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(cols[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   3348\u001B[0m     cols \u001B[38;5;241m=\u001B[39m cols[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m-> 3349\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq(cols, _to_java_column)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3336\u001B[0m, in \u001B[0;36mDataFrame._jseq\u001B[0;34m(self, cols, converter)\u001B[0m\n\u001B[1;32m   3330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_jseq\u001B[39m(\n\u001B[1;32m   3331\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   3332\u001B[0m     cols: Sequence,\n\u001B[1;32m   3333\u001B[0m     converter: Optional[Callable[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   3334\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001B[39;00m\n\u001B[0;32m-> 3336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _to_seq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39m_sc, cols, converter)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m_to_seq\u001B[0;34m(sc, cols, converter)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:106\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03mConvert a list of Columns (or names) into a JVM Seq of Column.\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03minto JVM Column objects.\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m converter:\n\u001B[0;32m--> 106\u001B[0m     cols \u001B[38;5;241m=\u001B[39m [converter(c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(cols)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:69\u001B[0m, in \u001B[0;36m_to_java_column\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m     67\u001B[0m     jcol \u001B[38;5;241m=\u001B[39m _create_column_from_name(col)\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 69\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m     70\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN_OR_STR\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     71\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m     72\u001B[0m     )\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m jcol\n",
        "\u001B[0;31mPySparkTypeError\u001B[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(100.0, 10.0), (200.0, 30.0), (50.0, 5.0), (70.0, 20.0)]\n",
    "df = spark.createDataFrame(data, [\"price\", \"discount\"])\n",
    "\n",
    "from pyspark.sql.functions import round, col, expr\n",
    "df.select(\"price\", \"discount\", round(expr(\"price - discount\")), 0).cast(\"int\") > 0;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9ef14a-3d47-4b8b-87de-8f7b741e099b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>price</th><th>discount</th><th>finalprice</th></tr></thead><tbody><tr><td>100.0</td><td>10.0</td><td>90</td></tr><tr><td>200.0</td><td>30.0</td><td>170</td></tr><tr><td>50.0</td><td>5.0</td><td>45</td></tr><tr><td>70.0</td><td>20.0</td><td>50</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100.0,
         10.0,
         90
        ],
        [
         200.0,
         30.0,
         170
        ],
        [
         50.0,
         5.0,
         45
        ],
        [
         70.0,
         20.0,
         50
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "discount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "finalprice",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(100.0, 10.0), (200.0, 30.0), (50.0, 5.0), (70.0, 20.0)]\n",
    "df = spark.createDataFrame(data, [\"price\", \"discount\"])\n",
    "\n",
    "from pyspark.sql.functions import round, col, expr\n",
    "display(df.select(\"price\", \"discount\", round(expr(\"price - discount\"), 0).cast(\"int\").alias(\"finalprice\")))\n",
    "\n",
    "\n",
    "# display(\n",
    "#   df.select(\n",
    "#     \"price\",\n",
    "#     \"discount\",\n",
    "#     round(expr(\"price - discount\"), 0).cast(\"int\").alias(\"finalprice\")\n",
    "#   )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-10-31 14:24:42",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}