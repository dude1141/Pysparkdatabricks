{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a423512-e164-41bb-bbd4-66065739c3d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: struct (nullable = true)\n |    |-- firstName: string (nullable = true)\n |    |-- middleName: string (nullable = true)\n |    |-- lastName: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "personData = [\n",
    "    ((\"James\", None, \"Smith\"), \"OH\", \"M\"),\n",
    "    ((\"Anna\", \"Rose\", \"\"), \"NY\", \"F\"),\n",
    "    ((\"Julia\", \"\", \"Williams\"), \"OH\", \"F\"),\n",
    "    ((\"Maria\", \"Anne\", \"Jones\"), \"NY\", \"M\"),\n",
    "    ((\"Jen\", \"Mary\", \"Brown\"), \"NY\", \"M\"),\n",
    "    ((\"Mike\", \"Mary\", \"Williams\"), \"OH\", \"M\")\n",
    "]\n",
    "\n",
    "personSchema=StructType([\n",
    "    StructField(\"Name\", StructType([\n",
    "        StructField(\"firstName\", StringType(), True),\n",
    "        StructField(\"middleName\", StringType(), True),\n",
    "        StructField(\"lastName\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True)\n",
    "])\n",
    "\n",
    "person_df=spark.createDataFrame(data=personData, schema=personSchema)\n",
    "person_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a58e58-dc29-4c3d-8ad0-34701afa2bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+------+-------------------+\n|OrderId|  Product|Quantity| Price|               Date|\n+-------+---------+--------+------+-------------------+\n|      1|   Laptop|       2|1500.0|2024-07-01 00:00:00|\n|      2|    Mouse|       5|  25.0|2024-07-02 00:00:00|\n|      3|  Monitor|       3| 300.0|2024-07-03 00:00:00|\n|      4| Keyboard|       4|  75.0|2024-07-04 00:00:00|\n|      5|   Mouse |       2|  25.0|               NULL|\n|      6| Monitor |       5| 300.0|2024-07-07 00:00:00|\n|      7|   Laptop|       3|1500.0|2024-07-09 00:00:00|\n|      8|    Mouse|       7|  NULL|2024-07-10 00:00:00|\n|      1|   Laptop|       3|1500.0|2024-07-01 00:00:00|\n|      2|    Mouse|       5|  25.0|2024-07-02 00:00:00|\n+-------+---------+--------+------+-------------------+\n\n+-------+---------+--------+------+-------------------+--------+\n|OrderId|  Product|Quantity| Price|               Date|discount|\n+-------+---------+--------+------+-------------------+--------+\n|      1|   Laptop|       2|1500.0|2024-07-01 00:00:00|     0.1|\n|      2|    Mouse|       5|  25.0|2024-07-02 00:00:00|     0.1|\n|      3|  Monitor|       3| 300.0|2024-07-03 00:00:00|     0.1|\n|      4| Keyboard|       4|  75.0|2024-07-04 00:00:00|     0.1|\n|      5|   Mouse |       2|  25.0|               NULL|     0.1|\n|      6| Monitor |       5| 300.0|2024-07-07 00:00:00|     0.1|\n|      7|   Laptop|       3|1500.0|2024-07-09 00:00:00|     0.1|\n|      8|    Mouse|       7|  NULL|2024-07-10 00:00:00|     0.1|\n|      1|   Laptop|       3|1500.0|2024-07-01 00:00:00|     0.1|\n|      2|    Mouse|       5|  25.0|2024-07-02 00:00:00|     0.1|\n+-------+---------+--------+------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"OrderId\", StringType(),True),\n",
    "  StructField(\"Product\", StringType(),True),\n",
    "  StructField(\"Quantity\", IntegerType(),True),\n",
    "  StructField(\"Price\", FloatType(),True),\n",
    "  StructField(\"Date\", StringType(),True)\n",
    "]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"Laptop\", 2, 1500.0, \"2024-07-01 00:00:00\"),\n",
    "    (\"2\", \"Mouse\", 5, 25.0, \"2024-07-02 00:00:00\"),\n",
    "    (\"3\", \"Monitor\", 3, 300.0, \"2024-07-03 00:00:00\"),\n",
    "    (\"4\", \"Keyboard\", 4, 75.0, \"2024-07-04 00:00:00\"),\n",
    "    (\"5\", \"Mouse \", 2, 25.0, None),\n",
    "    (\"6\", \" Monitor \", 5, 300.0, \"2024-07-07 00:00:00\"),\n",
    "    (\"7\", \"Laptop\", 3, 1500.0, \"2024-07-09 00:00:00\"),\n",
    "    (\"8\", \" Mouse\", 7, None, \"2024-07-10 00:00:00\"),\n",
    "    (\"1\", \"Laptop\", 3, 1500.0, \"2024-07-01 00:00:00\"),\n",
    "    (\"2\", \"Mouse\", 5, 25.0, \"2024-07-02 00:00:00\")\n",
    "]\n",
    "\n",
    "#Dataframe Creation\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "df.withColumn(\"discount\",lit(0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f010d0-cab8-4d8c-b67a-4e98167d3ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Product</th><th>sum((Quantity * price))</th></tr></thead><tbody><tr><td>Laptop</td><td>12000.0</td></tr><tr><td>Mouse</td><td>250.0</td></tr><tr><td>Keyboard</td><td>300.0</td></tr><tr><td>Monitor</td><td>900.0</td></tr><tr><td> Monitor </td><td>1500.0</td></tr><tr><td>Mouse </td><td>50.0</td></tr><tr><td> Mouse</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Laptop",
         12000.0
        ],
        [
         "Mouse",
         250.0
        ],
        [
         "Keyboard",
         300.0
        ],
        [
         "Monitor",
         900.0
        ],
        [
         " Monitor ",
         1500.0
        ],
        [
         "Mouse ",
         50.0
        ],
        [
         " Mouse",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sum((Quantity * price))",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderId</th><th>Product</th><th>Quantity</th><th>Price</th><th>Date</th><th>discountedprice</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>2</td><td>1500.0</td><td>2024-07-01 00:00:00</td><td>1350.0</td></tr><tr><td>2</td><td>Mouse</td><td>5</td><td>25.0</td><td>2024-07-02 00:00:00</td><td>22.5</td></tr><tr><td>3</td><td>Monitor</td><td>3</td><td>300.0</td><td>2024-07-03 00:00:00</td><td>270.0</td></tr><tr><td>4</td><td>Keyboard</td><td>4</td><td>75.0</td><td>2024-07-04 00:00:00</td><td>67.5</td></tr><tr><td>5</td><td>Mouse </td><td>2</td><td>25.0</td><td>null</td><td>22.5</td></tr><tr><td>6</td><td> Monitor </td><td>5</td><td>300.0</td><td>2024-07-07 00:00:00</td><td>270.0</td></tr><tr><td>7</td><td>Laptop</td><td>3</td><td>1500.0</td><td>2024-07-09 00:00:00</td><td>1350.0</td></tr><tr><td>8</td><td> Mouse</td><td>7</td><td>null</td><td>2024-07-10 00:00:00</td><td>null</td></tr><tr><td>1</td><td>Laptop</td><td>3</td><td>1500.0</td><td>2024-07-01 00:00:00</td><td>1350.0</td></tr><tr><td>2</td><td>Mouse</td><td>5</td><td>25.0</td><td>2024-07-02 00:00:00</td><td>22.5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Laptop",
         2,
         1500.0,
         "2024-07-01 00:00:00",
         1350.0
        ],
        [
         "2",
         "Mouse",
         5,
         25.0,
         "2024-07-02 00:00:00",
         22.5
        ],
        [
         "3",
         "Monitor",
         3,
         300.0,
         "2024-07-03 00:00:00",
         270.0
        ],
        [
         "4",
         "Keyboard",
         4,
         75.0,
         "2024-07-04 00:00:00",
         67.5
        ],
        [
         "5",
         "Mouse ",
         2,
         25.0,
         null,
         22.5
        ],
        [
         "6",
         " Monitor ",
         5,
         300.0,
         "2024-07-07 00:00:00",
         270.0
        ],
        [
         "7",
         "Laptop",
         3,
         1500.0,
         "2024-07-09 00:00:00",
         1350.0
        ],
        [
         "8",
         " Mouse",
         7,
         null,
         "2024-07-10 00:00:00",
         null
        ],
        [
         "1",
         "Laptop",
         3,
         1500.0,
         "2024-07-01 00:00:00",
         1350.0
        ],
        [
         "2",
         "Mouse",
         5,
         25.0,
         "2024-07-02 00:00:00",
         22.5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "discountedprice",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderId</th><th>Product</th><th>Quantity</th><th>Price</th><th>Date</th></tr></thead><tbody><tr><td>8</td><td> Mouse</td><td>7</td><td>null</td><td>2024-07-10 00:00:00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "8",
         " Mouse",
         7,
         null,
         "2024-07-10 00:00:00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum,when\n",
    "#display(df.withColumn(\"total value\",col(\"Quantity\")*col(\"price\")))\n",
    "\n",
    "display(df.groupBy(\"Product\").agg(sum(col(\"Quantity\")*col(\"price\"))))\n",
    "\n",
    "display(df.withColumn(\"discountedprice\",when(col(\"price\")>=25,col(\"price\")*0.9).otherwise(col(\"price\"))))\n",
    "\n",
    "display(df.filter(col(\"price\").isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6714f44a-1841-49af-8b87-539df424b502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#regular expressions\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "\n",
    "# Sample DataFrame with email addresses\n",
    "data = [(1,\"john1.doe@gmail.com\",), (2,\"jane2_smith@yahoo.com\",), (3,\"alice.joy3@example.com\",)]\n",
    "df = spark.createDataFrame(data, [\"id\",\"email\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c6aea2-4db9-4cfa-a047-f3dc5ea72428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+------+-------------------+\n|OrderId|  Product|Quantity| Price|               Date|\n+-------+---------+--------+------+-------------------+\n|      1|   Laptop|       2|1500.0|2024-07-01 00:00:00|\n|      2|    Mouse|       5|  25.0|2024-07-02 00:00:00|\n|      3|  Monitor|       3| 300.0|2024-07-03 00:00:00|\n|      4| Keyboard|       4|  75.0|2024-07-04 00:00:00|\n|      5|   Laptop|       1|1500.0|2024-07-05 00:00:00|\n|      6|   Mouse |       2|  25.0|               NULL|\n|      7| Monitor |       5| 300.0|2024-07-07 00:00:00|\n|      8| Keyboard|      10|  75.0|2024-07-08 00:00:00|\n|      9|   Laptop|       3|1500.0|2024-07-09 00:00:00|\n|     10|    Mouse|       7|  NULL|2024-07-10 00:00:00|\n+-------+---------+--------+------+-------------------+\n\nroot\n |-- OrderId: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: float (nullable = true)\n |-- Date: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderId</th><th>Product</th><th>Quantity</th><th>Price</th><th>Date</th><th>columndate</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>2</td><td>1500.0</td><td>2024-07-01 00:00:00</td><td>2025-11-02</td></tr><tr><td>2</td><td>Mouse</td><td>5</td><td>25.0</td><td>2024-07-02 00:00:00</td><td>2025-11-02</td></tr><tr><td>3</td><td>Monitor</td><td>3</td><td>300.0</td><td>2024-07-03 00:00:00</td><td>2025-11-02</td></tr><tr><td>4</td><td>Keyboard</td><td>4</td><td>75.0</td><td>2024-07-04 00:00:00</td><td>2025-11-02</td></tr><tr><td>5</td><td>Laptop</td><td>1</td><td>1500.0</td><td>2024-07-05 00:00:00</td><td>2025-11-02</td></tr><tr><td>6</td><td>Mouse </td><td>2</td><td>25.0</td><td>null</td><td>2025-11-02</td></tr><tr><td>7</td><td> Monitor </td><td>5</td><td>300.0</td><td>2024-07-07 00:00:00</td><td>2025-11-02</td></tr><tr><td>8</td><td>Keyboard</td><td>10</td><td>75.0</td><td>2024-07-08 00:00:00</td><td>2025-11-02</td></tr><tr><td>9</td><td>Laptop</td><td>3</td><td>1500.0</td><td>2024-07-09 00:00:00</td><td>2025-11-02</td></tr><tr><td>10</td><td> Mouse</td><td>7</td><td>null</td><td>2024-07-10 00:00:00</td><td>2025-11-02</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Laptop",
         2,
         1500.0,
         "2024-07-01 00:00:00",
         "2025-11-02"
        ],
        [
         "2",
         "Mouse",
         5,
         25.0,
         "2024-07-02 00:00:00",
         "2025-11-02"
        ],
        [
         "3",
         "Monitor",
         3,
         300.0,
         "2024-07-03 00:00:00",
         "2025-11-02"
        ],
        [
         "4",
         "Keyboard",
         4,
         75.0,
         "2024-07-04 00:00:00",
         "2025-11-02"
        ],
        [
         "5",
         "Laptop",
         1,
         1500.0,
         "2024-07-05 00:00:00",
         "2025-11-02"
        ],
        [
         "6",
         "Mouse ",
         2,
         25.0,
         null,
         "2025-11-02"
        ],
        [
         "7",
         " Monitor ",
         5,
         300.0,
         "2024-07-07 00:00:00",
         "2025-11-02"
        ],
        [
         "8",
         "Keyboard",
         10,
         75.0,
         "2024-07-08 00:00:00",
         "2025-11-02"
        ],
        [
         "9",
         "Laptop",
         3,
         1500.0,
         "2024-07-09 00:00:00",
         "2025-11-02"
        ],
        [
         "10",
         " Mouse",
         7,
         null,
         "2024-07-10 00:00:00",
         "2025-11-02"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "columndate",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderId</th><th>Product</th><th>Quantity</th><th>Price</th><th>Date</th><th>Datess</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>2</td><td>1500.0</td><td>2024-07-01 00:00:00</td><td>null</td></tr><tr><td>2</td><td>Mouse</td><td>5</td><td>25.0</td><td>2024-07-02 00:00:00</td><td>null</td></tr><tr><td>3</td><td>Monitor</td><td>3</td><td>300.0</td><td>2024-07-03 00:00:00</td><td>null</td></tr><tr><td>4</td><td>Keyboard</td><td>4</td><td>75.0</td><td>2024-07-04 00:00:00</td><td>null</td></tr><tr><td>5</td><td>Laptop</td><td>1</td><td>1500.0</td><td>2024-07-05 00:00:00</td><td>null</td></tr><tr><td>6</td><td>Mouse </td><td>2</td><td>25.0</td><td>null</td><td>null</td></tr><tr><td>7</td><td> Monitor </td><td>5</td><td>300.0</td><td>2024-07-07 00:00:00</td><td>null</td></tr><tr><td>8</td><td>Keyboard</td><td>10</td><td>75.0</td><td>2024-07-08 00:00:00</td><td>null</td></tr><tr><td>9</td><td>Laptop</td><td>3</td><td>1500.0</td><td>2024-07-09 00:00:00</td><td>null</td></tr><tr><td>10</td><td> Mouse</td><td>7</td><td>null</td><td>2024-07-10 00:00:00</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Laptop",
         2,
         1500.0,
         "2024-07-01 00:00:00",
         null
        ],
        [
         "2",
         "Mouse",
         5,
         25.0,
         "2024-07-02 00:00:00",
         null
        ],
        [
         "3",
         "Monitor",
         3,
         300.0,
         "2024-07-03 00:00:00",
         null
        ],
        [
         "4",
         "Keyboard",
         4,
         75.0,
         "2024-07-04 00:00:00",
         null
        ],
        [
         "5",
         "Laptop",
         1,
         1500.0,
         "2024-07-05 00:00:00",
         null
        ],
        [
         "6",
         "Mouse ",
         2,
         25.0,
         null,
         null
        ],
        [
         "7",
         " Monitor ",
         5,
         300.0,
         "2024-07-07 00:00:00",
         null
        ],
        [
         "8",
         "Keyboard",
         10,
         75.0,
         "2024-07-08 00:00:00",
         null
        ],
        [
         "9",
         "Laptop",
         3,
         1500.0,
         "2024-07-09 00:00:00",
         null
        ],
        [
         "10",
         " Mouse",
         7,
         null,
         "2024-07-10 00:00:00",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Datess",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"OrderId\", StringType(),True),\n",
    "  StructField(\"Product\", StringType(),True),\n",
    "  StructField(\"Quantity\", IntegerType(),True),\n",
    "  StructField(\"Price\", FloatType(),True),\n",
    "  StructField(\"Date\", StringType(),True)\n",
    "]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"Laptop\", 2, 1500.0, \"2024-07-01 00:00:00\"),\n",
    "    (\"2\", \"Mouse\", 5, 25.0, \"2024-07-02 00:00:00\"),\n",
    "    (\"3\", \"Monitor\", 3, 300.0, \"2024-07-03 00:00:00\"),\n",
    "    (\"4\", \"Keyboard\", 4, 75.0, \"2024-07-04 00:00:00\"),\n",
    "    (\"5\", \"Laptop\", 1, 1500.0, \"2024-07-05 00:00:00\"),\n",
    "    (\"6\", \"Mouse \", 2, 25.0, None),\n",
    "    (\"7\", \" Monitor \", 5, 300.0, \"2024-07-07 00:00:00\"),\n",
    "    (\"8\", \"Keyboard\", 10, 75.0, \"2024-07-08 00:00:00\"),\n",
    "    (\"9\", \"Laptop\", 3, 1500.0, \"2024-07-09 00:00:00\"),\n",
    "    (\"10\", \" Mouse\", 7, None, \"2024-07-10 00:00:00\")\n",
    "]\n",
    "\n",
    "#Dataframe Creation\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "#show(): Display the DataFrame\n",
    "df.show()\n",
    "\n",
    "#printSchema(): Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import current_date\n",
    "display(df.withColumn(\"columndate\",current_date()))\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "display(df.withColumn(\"Datess\",to_date(col(\"Date\"),\"yyyy-MM-DD HH:mm:ss\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db02800-dbd2-4f8c-8971-011b6f7c207c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762053223877}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Datesadd</th></tr></thead><tbody><tr><td>2024-07-01 00:00:00</td><td>2024-07-06</td></tr><tr><td>2024-07-02 00:00:00</td><td>2024-07-07</td></tr><tr><td>2024-07-03 00:00:00</td><td>2024-07-08</td></tr><tr><td>2024-07-04 00:00:00</td><td>2024-07-09</td></tr><tr><td>2024-07-05 00:00:00</td><td>2024-07-10</td></tr><tr><td>null</td><td>null</td></tr><tr><td>2024-07-07 00:00:00</td><td>2024-07-12</td></tr><tr><td>2024-07-08 00:00:00</td><td>2024-07-13</td></tr><tr><td>2024-07-09 00:00:00</td><td>2024-07-14</td></tr><tr><td>2024-07-10 00:00:00</td><td>2024-07-15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-07-01 00:00:00",
         "2024-07-06"
        ],
        [
         "2024-07-02 00:00:00",
         "2024-07-07"
        ],
        [
         "2024-07-03 00:00:00",
         "2024-07-08"
        ],
        [
         "2024-07-04 00:00:00",
         "2024-07-09"
        ],
        [
         "2024-07-05 00:00:00",
         "2024-07-10"
        ],
        [
         null,
         null
        ],
        [
         "2024-07-07 00:00:00",
         "2024-07-12"
        ],
        [
         "2024-07-08 00:00:00",
         "2024-07-13"
        ],
        [
         "2024-07-09 00:00:00",
         "2024-07-14"
        ],
        [
         "2024-07-10 00:00:00",
         "2024-07-15"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Datesadd",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "display(df.select(\"Date\").withColumn(\"Datesadd\",date_add(col(\"Date\"),5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165de8cd-50a6-4957-8d22-e2852b9a8868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-8895425144103781>, line 6\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    display(df.select(\"Date\").withColumn(\"unixtimestamp\",unix_timestamp(col(\"Date\"),\"yyyy-MM-dd HH:mm:ss\"))\u001B[0m\n",
       "\u001B[0m                                                                                                           ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "incomplete input (command-8895425144103781-3986908454, line 6)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: incomplete input (command-8895425144103781-3986908454, line 6)"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-8895425144103781>, line 6\u001B[0;36m\u001B[0m\n\u001B[0;31m    display(df.select(\"Date\").withColumn(\"unixtimestamp\",unix_timestamp(col(\"Date\"),\"yyyy-MM-dd HH:mm:ss\"))\u001B[0m\n\u001B[0m                                                                                                           ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.select(\"Date\").withColumn(\"Datesadd\",date_add(col(\"Date\"),5)))\n",
    "\n",
    "\n",
    "#convert to unixtimestamp\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "display(df.select(\"Date\").withColumn(\"unixtimestamp\",unix_timestamp(col(\"Date\"),\"yyyy-MM-dd HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac060987-3dfc-4345-a353-50c84f6f049a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>Datesadd</th></tr></thead><tbody><tr><td>2024-07-01 00:00:00</td><td>2024-07-06</td></tr><tr><td>2024-07-02 00:00:00</td><td>2024-07-07</td></tr><tr><td>2024-07-03 00:00:00</td><td>2024-07-08</td></tr><tr><td>2024-07-04 00:00:00</td><td>2024-07-09</td></tr><tr><td>2024-07-05 00:00:00</td><td>2024-07-10</td></tr><tr><td>null</td><td>null</td></tr><tr><td>2024-07-07 00:00:00</td><td>2024-07-12</td></tr><tr><td>2024-07-08 00:00:00</td><td>2024-07-13</td></tr><tr><td>2024-07-09 00:00:00</td><td>2024-07-14</td></tr><tr><td>2024-07-10 00:00:00</td><td>2024-07-15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-07-01 00:00:00",
         "2024-07-06"
        ],
        [
         "2024-07-02 00:00:00",
         "2024-07-07"
        ],
        [
         "2024-07-03 00:00:00",
         "2024-07-08"
        ],
        [
         "2024-07-04 00:00:00",
         "2024-07-09"
        ],
        [
         "2024-07-05 00:00:00",
         "2024-07-10"
        ],
        [
         null,
         null
        ],
        [
         "2024-07-07 00:00:00",
         "2024-07-12"
        ],
        [
         "2024-07-08 00:00:00",
         "2024-07-13"
        ],
        [
         "2024-07-09 00:00:00",
         "2024-07-14"
        ],
        [
         "2024-07-10 00:00:00",
         "2024-07-15"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Datesadd",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date</th><th>unixtimestamp</th></tr></thead><tbody><tr><td>2024-07-01 00:00:00</td><td>1719792000</td></tr><tr><td>2024-07-02 00:00:00</td><td>1719878400</td></tr><tr><td>2024-07-03 00:00:00</td><td>1719964800</td></tr><tr><td>2024-07-04 00:00:00</td><td>1720051200</td></tr><tr><td>2024-07-05 00:00:00</td><td>1720137600</td></tr><tr><td>null</td><td>null</td></tr><tr><td>2024-07-07 00:00:00</td><td>1720310400</td></tr><tr><td>2024-07-08 00:00:00</td><td>1720396800</td></tr><tr><td>2024-07-09 00:00:00</td><td>1720483200</td></tr><tr><td>2024-07-10 00:00:00</td><td>1720569600</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-07-01 00:00:00",
         1719792000
        ],
        [
         "2024-07-02 00:00:00",
         1719878400
        ],
        [
         "2024-07-03 00:00:00",
         1719964800
        ],
        [
         "2024-07-04 00:00:00",
         1720051200
        ],
        [
         "2024-07-05 00:00:00",
         1720137600
        ],
        [
         null,
         null
        ],
        [
         "2024-07-07 00:00:00",
         1720310400
        ],
        [
         "2024-07-08 00:00:00",
         1720396800
        ],
        [
         "2024-07-09 00:00:00",
         1720483200
        ],
        [
         "2024-07-10 00:00:00",
         1720569600
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "unixtimestamp",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.select(\"Date\").withColumn(\"Datesadd\",date_add(col(\"Date\"),5)))\n",
    "\n",
    "\n",
    "#convert to unixtimestamp\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "display(df.select(\"Date\").withColumn(\"unixtimestamp\",unix_timestamp(col(\"Date\"),\"yyyy-MM-dd HH:mm:ss\")))\n",
    "\n",
    "#from unixtimestamp to date using from_unixtimestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a4064e-1b8d-4a81-a89a-0b7cdb1bcafa",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762054147814}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----+\n|   Category|Product|Sales|\n+-----------+-------+-----+\n|Electronics| Laptop| 1000|\n|Electronics|  Mouse|   50|\n|  Furniture|   Desk|  300|\n|  Furniture|  Chair|  150|\n+-----------+-------+-----+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Category</th><th>sum(Sales)</th></tr></thead><tbody><tr><td>Electronics</td><td>1050</td></tr><tr><td>Furniture</td><td>450</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Electronics",
         1050
        ],
        [
         "Furniture",
         450
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sum(Sales)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the DataFrame for applying Pivot and Unpivot fuctions\n",
    "data = [\n",
    "    ('Electronics', 'Laptop', 1000),\n",
    "    ('Electronics', 'Mouse', 50),\n",
    "    ('Furniture', 'Desk', 300),\n",
    "    ('Furniture', 'Chair', 150)\n",
    "]\n",
    "\n",
    "columns = ['Category', 'Product', 'Sales']\n",
    "pivot = spark.createDataFrame(data, columns)\n",
    "\n",
    "pivot.show()\n",
    "\n",
    "#pivot\n",
    "\n",
    "#from pyspark.sql.functions import pivot\n",
    "display(pivot.groupBy(\"Category\").agg(sum(\"Sales\")))\n",
    "# display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1496152a-1220-4386-8321-84863134f575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n|   dept|male|female|\n+-------+----+------+\n|     IT|   8|     5|\n|Payroll|   3|     6|\n|     HR|   7|     7|\n+-------+----+------+\n\n+-------+------+-----+\n|   dept|gender|count|\n+-------+------+-----+\n|     IT|     M|    8|\n|     IT|     F|    5|\n|Payroll|     M|    3|\n|Payroll|     F|    6|\n|     HR|     M|    7|\n|     HR|     F|    7|\n+-------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#unpivot\n",
    "\n",
    "\n",
    "data = [('IT',8,5),('Payroll',3,6),('HR',7,7)]\n",
    "\n",
    "unpivot = spark.createDataFrame(data,['dept','male','female'])\n",
    "unpivot.show()\n",
    "\n",
    "from pyspark.sql.functions import stack, col, expr\n",
    "unpvt=unpivot.select(\"dept\",expr(\"stack(2, 'M', male, 'F', female) as (gender, count)\"))\n",
    "unpvt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1247c48c-2889-4a86-b741-d29fd929dc2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8895425144104403>, line 13\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Define schema\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[1;32m      6\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[1;32m      7\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProduct\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     11\u001B[0m ])\n",
       "\u001B[0;32m---> 13\u001B[0m data\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPERMISSIVE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mschema(schema)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/sample_orders.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     15\u001B[0m df\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;66;03m#spark.createDataFrame(data, schema)\u001B[39;00m\n",
       "\u001B[1;32m     16\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:312\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    310\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mload(path))\n",
       "\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/sample_orders.csv. SQLSTATE: 42K03"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/sample_orders.csv. SQLSTATE: 42K03"
       },
       "metadata": {
        "errorSummary": "[PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/sample_orders.csv. SQLSTATE: 42K03"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "PATH_NOT_FOUND",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "42K03",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-8895425144104403>, line 13\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Define schema\u001B[39;00m\n\u001B[1;32m      5\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m      6\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m, IntegerType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      7\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProduct\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDate\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     11\u001B[0m ])\n\u001B[0;32m---> 13\u001B[0m data\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPERMISSIVE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mschema(schema)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/sample_orders.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m df\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;66;03m#spark.createDataFrame(data, schema)\u001B[39;00m\n\u001B[1;32m     16\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:312\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mload(path))\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/sample_orders.csv. SQLSTATE: 42K03"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), True),\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Quantity\", StringType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Date\", StringType(), True)\n",
    "])\n",
    "\n",
    "data=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"\",\"PERMISSIVE\").schema(schema).load('dbfs:/FileStore/mithlesh123adf/employee.csv')\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c677deb7-19cd-4d9f-88a3-569d8ec88217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9e88931d-87fb-48a1-84f0-91d229bf9d56/lib/python3.11/site-packages (3.1.5)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.11/site-packages (1.5.3)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9e88931d-87fb-48a1-84f0-91d229bf9d56/lib/python3.11/site-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas) (2022.7)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.11/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderID</th><th>Product</th><th>Quantity</th><th>Price</th><th>Date</th><th>Unnamed: 5</th></tr></thead><tbody><tr><td>1</td><td>Laptop</td><td>2</td><td>1500</td><td>2024-07-01</td><td>null</td></tr><tr><td>2</td><td>Mouse</td><td>5</td><td>25</td><td>2024-07-02</td><td>null</td></tr><tr><td>3</td><td>Monitor</td><td>3</td><td>300</td><td>2024-07-03</td><td>null</td></tr><tr><td>4</td><td>Keyboard</td><td>4</td><td>75</td><td>2024-07-04</td><td>null</td></tr><tr><td>5</td><td>Laptop</td><td>1</td><td>1500</td><td>2024-07-05</td><td>null</td></tr><tr><td>6</td><td>Headphones</td><td>2</td><td>120</td><td>2024-07-06</td><td>null</td></tr><tr><td>7</td><td>Webcam</td><td>3</td><td>80</td><td>2024-07-07</td><td>null</td></tr><tr><td>8</td><td>Printer</td><td>1</td><td>450</td><td>2024-07-08</td><td>null</td></tr><tr><td>9</td><td>Mousepad</td><td>6</td><td>10</td><td>2024-07-09</td><td>null</td></tr><tr><td>10</td><td>Speaker</td><td>2</td><td>200</td><td>2024-07-10</td><td>null</td></tr><tr><td>11</td><td>null</td><td>3</td><td>250</td><td>2024/07/11</td><td>null</td></tr><tr><td>12A</td><td>Laptop</td><td>two</td><td>fifteen hundred</td><td>2024-07-12</td><td>null</td></tr><tr><td>13</td><td>Monitor</td><td>-2</td><td>300</td><td>2024-07-13</td><td>null</td></tr><tr><td>null</td><td>Mouse</td><td>4</td><td>25</td><td>2024-07-14</td><td>null</td></tr><tr><td>14</td><td>Keyboard</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>15</td><td>Laptop</td><td>1</td><td>1500</td><td>NotADate</td><td>null</td></tr><tr><td>16</td><td>Tablet</td><td>2</td><td>null</td><td>2024-07-16</td><td>null</td></tr><tr><td>17</td><td>Camera</td><td>3x</td><td>500</td><td>2024-07-17</td><td>null</td></tr><tr><td>18</td><td>Phone</td><td>2</td><td>999</td><td>2024-07-18</td><td>ExtraColumn</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "Laptop",
         "2",
         "1500",
         "2024-07-01",
         null
        ],
        [
         "2",
         "Mouse",
         "5",
         "25",
         "2024-07-02",
         null
        ],
        [
         "3",
         "Monitor",
         "3",
         "300",
         "2024-07-03",
         null
        ],
        [
         "4",
         "Keyboard",
         "4",
         "75",
         "2024-07-04",
         null
        ],
        [
         "5",
         "Laptop",
         "1",
         "1500",
         "2024-07-05",
         null
        ],
        [
         "6",
         "Headphones",
         "2",
         "120",
         "2024-07-06",
         null
        ],
        [
         "7",
         "Webcam",
         "3",
         "80",
         "2024-07-07",
         null
        ],
        [
         "8",
         "Printer",
         "1",
         "450",
         "2024-07-08",
         null
        ],
        [
         "9",
         "Mousepad",
         "6",
         "10",
         "2024-07-09",
         null
        ],
        [
         "10",
         "Speaker",
         "2",
         "200",
         "2024-07-10",
         null
        ],
        [
         "11",
         null,
         "3",
         "250",
         "2024/07/11",
         null
        ],
        [
         "12A",
         "Laptop",
         "two",
         "fifteen hundred",
         "2024-07-12",
         null
        ],
        [
         "13",
         "Monitor",
         "-2",
         "300",
         "2024-07-13",
         null
        ],
        [
         null,
         "Mouse",
         "4",
         "25",
         "2024-07-14",
         null
        ],
        [
         "14",
         "Keyboard",
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "Laptop",
         "1",
         "1500",
         "NotADate",
         null
        ],
        [
         "16",
         "Tablet",
         "2",
         null,
         "2024-07-16",
         null
        ],
        [
         "17",
         "Camera",
         "3x",
         "500",
         "2024-07-17",
         null
        ],
        [
         "18",
         "Phone",
         "2",
         "999",
         "2024-07-18",
         "ExtraColumn"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Unnamed: 5",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use /dbfs/ prefix for pandas to access files uploaded to DBFS\n",
    "excel_path = \"/dbfs/FileStore/mithlesh123adf/sample_orders_with_issues.xlsx\"\n",
    "csv_path = \"/dbfs/FileStore/mithlesh123adf/orders_temp.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%pip install openpyxl pandas\n",
    "\n",
    "pd.read_excel(excel_path).to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "# Use dbfs:/ prefix for Spark to access the same file\n",
    "df = spark.read.csv(\n",
    "    \"dbfs:/FileStore/mithlesh123adf/orders_temp.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "# display(df)\n",
    "\n",
    "\n",
    "df1 = spark.read.csv(\n",
    "    \"dbfs:/FileStore/mithlesh123adf/orders_temp.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    mode=\"DROPMALFORMED\"\n",
    ")\n",
    "display(df1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdec002-d017-4870-9d24-08633cb75bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[Row(emp_id=1, emp_name='John Doe', salary=50000.0),\n",
       "  Row(emp_id=4, emp_name='Lisa Ray', salary=52000.0),\n",
       "  Row(emp_id=7, emp_name='Gary Black', salary=45000.0),\n",
       "  Row(emp_id=9, emp_name='Harry Pink', salary=53000.0),\n",
       "  Row(emp_id=12, emp_name='Katie Purple', salary=57000.0),\n",
       "  Row(emp_id=10, emp_name='Megan Yellow', salary=58000.0)],\n",
       " [Row(emp_id=2, emp_name='Jane Smith', salary=55000.0),\n",
       "  Row(emp_id=3, emp_name='Sam Brown', salary=60000.0),\n",
       "  Row(emp_id=6, emp_name='Nancy Green', salary=62000.0),\n",
       "  Row(emp_id=5, emp_name='Tom White', salary=48000.0),\n",
       "  Row(emp_id=8, emp_name='Monica Blue', salary=47000.0),\n",
       "  Row(emp_id=11, emp_name='Chris Red', salary=61000.0)]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "data = [\n",
    "    (1, \"John Doe\", 50000.0),\n",
    "    (2, \"Jane Smith\", 55000.0),\n",
    "    (3, \"Sam Brown\", 60000.0),\n",
    "    (4, \"Lisa Ray\", 52000.0),\n",
    "    (5, \"Tom White\", 48000.0),\n",
    "    (6, \"Nancy Green\", 62000.0),\n",
    "    (7, \"Gary Black\", 45000.0),\n",
    "    (8, \"Monica Blue\", 47000.0),\n",
    "    (9, \"Harry Pink\", 53000.0),\n",
    "    (10, \"Megan Yellow\", 58000.0),\n",
    "    (11, \"Chris Red\", 61000.0),\n",
    "    (12, \"Katie Purple\", 57000.0)\n",
    "]\n",
    "\n",
    "# Define the column names\n",
    "columns = [\"emp_id\", \"emp_name\", \"salary\"]\n",
    "    \n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "# df.show(truncate=False)\n",
    "\n",
    "df.rdd.getNumPartitions()\n",
    "\n",
    "# df.rdd.glom().collect()\n",
    "\n",
    "df2=df.repartition(2)\n",
    "df2.rdd.getNumPartitions()\n",
    "df2.rdd.glom().collect()\n",
    "\n",
    "# df3=df.coalesce(2)\n",
    "# df3.rdd.glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3bd776-aca3-4c9a-ae17-7b73e1423c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>designation</th><th>dept_id</th><th>dept_id</th><th>dept_name</th></tr></thead><tbody><tr><td>103</td><td>Ravi</td><td>Analyst</td><td>null</td><td>null</td><td>null</td></tr><tr><td>101</td><td>Arun</td><td>Manager</td><td>1</td><td>1</td><td>HR</td></tr><tr><td>105</td><td>Kiran</td><td>Developer</td><td>2</td><td>2</td><td>IT</td></tr><tr><td>106</td><td>Priya</td><td>Analyst</td><td>3</td><td>3</td><td>Finance</td></tr><tr><td>104</td><td>Neha</td><td>Manager</td><td>4</td><td>4</td><td>Marketing</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>5</td><td>Sales</td></tr><tr><td>107</td><td>Priya</td><td>Analyst</td><td>6</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         103,
         "Ravi",
         "Analyst",
         null,
         null,
         null
        ],
        [
         101,
         "Arun",
         "Manager",
         1,
         1,
         "HR"
        ],
        [
         105,
         "Kiran",
         "Developer",
         2,
         2,
         "IT"
        ],
        [
         106,
         "Priya",
         "Analyst",
         3,
         3,
         "Finance"
        ],
        [
         104,
         "Neha",
         "Manager",
         4,
         4,
         "Marketing"
        ],
        [
         null,
         null,
         null,
         null,
         5,
         "Sales"
        ],
        [
         107,
         "Priya",
         "Analyst",
         6,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "designation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>designation</th><th>dept_id</th></tr></thead><tbody><tr><td>101</td><td>Arun</td><td>Manager</td><td>1</td></tr><tr><td>105</td><td>Kiran</td><td>Developer</td><td>2</td></tr><tr><td>106</td><td>Priya</td><td>Analyst</td><td>3</td></tr><tr><td>104</td><td>Neha</td><td>Manager</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "Arun",
         "Manager",
         1
        ],
        [
         105,
         "Kiran",
         "Developer",
         2
        ],
        [
         106,
         "Priya",
         "Analyst",
         3
        ],
        [
         104,
         "Neha",
         "Manager",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "designation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>designation</th><th>dept_id</th></tr></thead><tbody><tr><td>103</td><td>Ravi</td><td>Analyst</td><td>null</td></tr><tr><td>107</td><td>Priya</td><td>Analyst</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         103,
         "Ravi",
         "Analyst",
         null
        ],
        [
         107,
         "Priya",
         "Analyst",
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "designation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# jooins\n",
    "\n",
    "employee_data = [\n",
    "    (101, \"Arun\", \"Manager\", 1),\n",
    "    (103, \"Ravi\", \"Analyst\", None),\n",
    "    (104, \"Neha\", \"Manager\", 4),\n",
    "    (105, \"Kiran\", \"Developer\", 2),\n",
    "    (106, \"Priya\", \"Analyst\", 3),\n",
    "    (107, \"Priya\", \"Analyst\", 6)\n",
    "]\n",
    "\n",
    "# Sample data for dept_df\n",
    "dept_data = [\n",
    "    (1, \"HR\"),\n",
    "    (2, \"IT\"),\n",
    "    (3, \"Finance\"),\n",
    "    (4, \"Marketing\"),\n",
    "    (5, \"Sales\")\n",
    "]\n",
    "\n",
    "# Define schema for DataFrames\n",
    "employee_columns = [\"emp_id\", \"name\", \"designation\", \"dept_id\"]\n",
    "dept_columns = [\"dept_id\", \"dept_name\"]\n",
    "\n",
    "# Create DataFrames\n",
    "a = spark.createDataFrame(employee_data, employee_columns)\n",
    "b = spark.createDataFrame(dept_data, dept_columns)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "# Show the DataFrames\n",
    "# employee_df.show()\n",
    "# dept_df.show()\n",
    "empdf=a.join(b,a[\"dept_id\"]==b[\"dept_id\"],\"fullouter\")\n",
    "\n",
    "#inner join vs left semi---left semi dont bring RECORDS from right table\n",
    "display(empdf)\n",
    "\n",
    "empdf1=a.join(b,a[\"dept_id\"]==b[\"dept_id\"],\"left_semi\")\n",
    "display(empdf1)\n",
    "\n",
    "\n",
    "empdf12=a.join(b,a[\"dept_id\"]==b[\"dept_id\"],\"left_anti\") #BRING DATA FROM left that does not have matches to right\n",
    "display(empdf12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b01151-3208-4c86-87b1-2b8dd6152715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+------+\n|employee_id|       name|department|salary|\n+-----------+-----------+----------+------+\n|          1|Alice Smith| Marketing|  6800|\n|          2|Carol White|        HR|  8000|\n|          3|  Bob Brown|        IT|  7800|\n+-----------+-----------+----------+------+\n\n+-----------+-----------+----------+------+\n|employee_id|       name|department|salary|\n+-----------+-----------+----------+------+\n|          1|Alice Smith| Marketing|  6800|\n|          4|David Green|     Sales|  7200|\n+-----------+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "target_data = [\n",
    "    (1, 'Alice Smith', 'Marketing', 6800),\n",
    "    (2, 'Carol White', 'HR', 8000),\n",
    "    (3, 'Bob Brown', 'IT', 7800)\n",
    "]\n",
    "schema = [\"employee_id\", \"name\", \"department\", \"salary\"]\n",
    "target_df = spark.createDataFrame(target_data,schema)\n",
    "target_df.show()\n",
    "\n",
    "\n",
    "source_data = [\n",
    "    (1, 'Alice Smith', 'Marketing', 6800),\n",
    "    (4, 'David Green', 'Sales', 7200),\n",
    "]\n",
    "schema = [\"employee_id\", \"name\", \"department\", \"salary\"]\n",
    "source_df = spark.createDataFrame(source_data,schema)\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c28877-f1cb-43f8-bace-298f44fd4830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+------+\n|employee_id|name       |department|salary|\n+-----------+-----------+----------+------+\n|4          |David Green|Sales     |7200  |\n+-----------+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "records_upsert = source_df.join(target_df, on=[\"employee_id\"],how=\"left_anti\")\n",
    "records_upsert.show(truncate=False)\n",
    "# final_df = records_upsert.drop(\"hash64\")\n",
    "# final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e4289e-e9dd-4047-b3fb-be8f31218740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+------+\n|employee_id|       name|department|salary|\n+-----------+-----------+----------+------+\n|          1|Alice Smith| Marketing|  6800|\n|          2|Carol White|        HR|  7000|\n|          3|  Bob Brown|        IT|  7800|\n+-----------+-----------+----------+------+\n\n+-----------+-----------+----------+------+----------+\n|employee_id|name       |department|salary|hash64    |\n+-----------+-----------+----------+------+----------+\n|1          |Alice Smith|Marketing |6800  |95159712  |\n|2          |Carol White|HR        |8000  |-792810495|\n|3          |Bob Brown  |IT        |7800  |717746182 |\n|4          |David Green|Sales     |7200  |-471694084|\n+-----------+-----------+----------+------+----------+\n\n+-----------+-----------+----------+------+-----------+\n|employee_id|name       |department|salary|hash64     |\n+-----------+-----------+----------+------+-----------+\n|1          |Alice Smith|Marketing |6800  |95159712   |\n|2          |Carol White|HR        |7000  |-2004201978|\n|3          |Bob Brown  |IT        |7800  |717746182  |\n+-----------+-----------+----------+------+-----------+\n\n+----------+-----------+-----------+----------+------+\n|hash64    |employee_id|name       |department|salary|\n+----------+-----------+-----------+----------+------+\n|-792810495|2          |Carol White|HR        |8000  |\n|-471694084|4          |David Green|Sales     |7200  |\n+----------+-----------+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# existing data in the target table\n",
    "data_target = [\n",
    "    (1, \"Alice Smith\", \"Marketing\", 6800),\n",
    "    (2, \"Carol White\", \"HR\", 7000),\n",
    "    (3, \"Bob Brown\", \"IT\", 7800)\n",
    "]\n",
    "cols = [\"employee_id\", \"name\", \"department\", \"salary\"]\n",
    "\n",
    "df_target = spark.createDataFrame(data_target, cols)\n",
    "df_target.show()\n",
    "\n",
    "\n",
    "data_source = [\n",
    "    (1, \"Alice Smith\", \"Marketing\", 6800),  # same\n",
    "    (2, \"Carol White\", \"HR\", 8000),         # salary changed\n",
    "    (3, \"Bob Brown\", \"IT\", 7800),           # same\n",
    "    (4, \"David Green\", \"Sales\", 7200)       # new record\n",
    "]\n",
    "\n",
    "df_source = spark.createDataFrame(data_source, cols)\n",
    "# df_source.show()\n",
    "\n",
    "leftantidf=df_source.join(df_target, on=[\"employee_id\"],how=\"left_anti\")\n",
    "# leftantidf.show(truncate=False)\n",
    "\n",
    "#lets use hashfunction\n",
    "df_src=df_source.withColumn(\"hash64\", F.hash(F.concat(F.col(\"employee_id\"), F.col(\"name\")), F.col(\"department\"), F.col(\"salary\")))\n",
    "\n",
    "df_tgt=df_target.withColumn(\"hash64\", F.hash(F.concat(F.col(\"employee_id\"), F.col(\"name\")), F.col(\"department\"), F.col(\"salary\")))\n",
    "\n",
    "df_src.show(truncate=False)\n",
    "df_tgt.show(truncate=False)\n",
    "\n",
    "df_src.join(df_tgt, on=[\"hash64\"],how=\"left_anti\").show(truncate=False)\n",
    "#without hashing we mis carolwhite as left anti on employee_id ignores carlowhite\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-01 22:25:03",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}