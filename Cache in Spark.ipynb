{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "450a136e-55ce-41dd-a9c6-2be22e0da7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### *Cache*\n",
    "\n",
    "*Caching a DataFrame in PySpark is a performance optimization technique that stores the DataFrame's contents in memory or disk for faster access during subsequent operations.*\n",
    "\n",
    "##### *Performance Improvement:* \n",
    "*Caching a DataFrame reduces the need to recompute the DataFrame's transformations, especially when the DataFrame is used multiple times in subsequent operations. This can significantly improve performance, especially for iterative algorithms or complex computations.*\n",
    "\n",
    "##### *Caching Syntax:* \n",
    "*To cache a DataFrame, you can use the cache() method. For example:* ***df.cache()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4366f85-b867-42be-90ac-30389394e4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab13d9f-c6c3-4453-a01f-bd38c6555014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs('/FileStore/mithelesh/inbound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da6797b-9753-44b2-87e4-f9dccc2a35ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Without Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "315b8290-0481-4e03-9406-d949cc833ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|  user_id|total_amount|\n+---------+------------+\n|555447570|      1137.0|\n|524493281|      1432.0|\n|543091275|      9047.0|\n|509481306|       180.0|\n|513992906|       165.0|\n+---------+------------+\nonly showing top 5 rows\nExecution time without cache: 14.392612218856812 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, date_format, round, sum\n",
    "from time import time\n",
    "# file_location = \"/FileStore/tables/sales126mb-1.csv\"\n",
    "\n",
    "# Step 1: Load Data\n",
    "try:\n",
    "    df = spark.read.csv('/FileStore/mithelesh/inbound/sales126mb.csv', header=True, inferSchema=True)\n",
    "    \n",
    "    # Step 2: Creating dataframe\n",
    "    df1 = df.withColumn(\"order_date\", date_format(col(\"event_time\"), \"yyyy-MM-dd\")).select(\"user_id\", \"product_id\", \"brand\", \"price\", \"order_date\")  \n",
    "    #df2 = df1\n",
    "    # Step 3: Measure execution time without caching\n",
    "    start_time = time()\n",
    "    df_without_cache = df1.groupBy(\"user_id\").agg(round(sum(\"price\")).alias(\"total_amount\"))\n",
    "    df_without_cache.show(5)\n",
    "    #df_without_cache.count()  # Trigger an action to force computation\n",
    "    time_without_cache = time() - start_time\n",
    "    print(f\"Execution time without cache: {time_without_cache} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e1a4a5-7e4d-4cea-9759-eade6289706b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read or scan data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11519c59-441f-4dc9-b2d0-34a7080542cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](files/images/image_before_cache.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf9ba057-03fe-4d60-a542-0a7e57ccf12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### No data cached in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78875e9-929e-4ec7-a005-8b1907040899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](files/images/image_before_cache_storage_stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3ed57d-4717-47ea-b1ef-4659ee898f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#With Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d85bf5ea-0246-4613-b59b-fa11cbba5df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|  user_id|total_amount|\n+---------+------------+\n|555447570|      1137.0|\n|524493281|      1432.0|\n|543091275|      9047.0|\n|509481306|       180.0|\n|513992906|       165.0|\n+---------+------------+\nonly showing top 5 rows\nExecution time with cache: 3.6317877769470215 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, round, sum\n",
    "from time import time\n",
    "\n",
    "# Step 1: Load Data\n",
    "try:\n",
    "    df = spark.read.csv(\"/FileStore/mithelesh/inbound/sales126mb.csv\", header=True, inferSchema=True)\n",
    "    \n",
    "    # Step 2: Process Data (without caching)\n",
    "    df1 = df.withColumn(\"order_date\", date_format(col(\"event_time\"), \"yyyy-MM-dd\")).select(\"user_id\", \"product_id\", \"brand\", \"price\", \"order_date\")  \n",
    "         \n",
    "    # Step 4: Process Data (with caching)\n",
    "    df1.cache()  # Cache the DataFrame in memory\n",
    "    df_with_cache = df1.groupBy(\"user_id\").agg(round(sum(\"price\")).alias(\"total_amount\"))\n",
    "    # Step 5: Measure execution time with caching\n",
    "    start_time = time()\n",
    "    df_without_cache.show(5)\n",
    "    #df_with_cache.count()  # Trigger an action to force computation\n",
    "    time_with_cache = time() - start_time\n",
    "    print(f\"Execution time with cache: {time_with_cache} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f063cbb-b842-4e22-93d1-9abf3ab8e5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f811d0eb-855c-432f-aedb-2a7d3f523394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Read or scan data from memory instead of disk after cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f3460c-bf9e-4ebc-8ce6-523694d2be22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](files/images/image_after_cache.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b59a785-e72d-4160-a110-7703e4fd7644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data cached in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f42f041-2253-4573-a25e-7f54bb16b713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](files/images/image_after_cache_storage_stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "237e619a-c7d2-4032-aee2-61485a267d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Storage Level: Disk Memory Deserialized 1x Replicated\n",
    "\n",
    "**Description:**\n",
    "- **Disk Memory**: Data is stored in memory (RAM) if available, and spilled to disk if there is insufficient memory. This ensures data retention even if memory is full, though accessing disk-stored data is slower.\n",
    "- **Deserialized**: Data is stored in its original object format in memory, which allows for faster access as it doesn't require deserialization. This typically uses more memory than serialized data.\n",
    "- **1x Replicated**: Data is stored without replication, meaning each partition is stored on a single node only. This saves memory and disk space but does not provide redundancy in case of node failure.\n",
    "\n",
    "**Use Case:**\n",
    "This storage level is useful when:\n",
    "- You need to balance between memory usage and persistence.\n",
    "- Fast access to data is required, and there is sufficient memory available.\n",
    "- Redundancy is not a primary concern, and you're comfortable with recomputing data if a node fails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30fcc79a-4de9-46fa-822b-93f1de4eff6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### *Here's a summary of few more DataFrame caching points*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38660541-5007-4271-9529-ab94efbb061f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### *Unpersisting:* \n",
    "*When the cached DataFrame is no longer needed or when memory resources are required, you can unpersist it using the unpersist() method:*\n",
    "\n",
    "###### *df.unpersist()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0644fd5-1009-4e56-8a9b-38f5d2053197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, product_id: int, brand: string, price: double, order_date: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up\n",
    "df1.unpersist()  # Unpersist the cached DataFrame from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631cb0f6-8030-4598-8e05-96750207f70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####*Cache Management:* \n",
    "*It's essential to manage DataFrame caching efficiently to avoid memory issues. Caching too many DataFrames or caching large DataFrames can lead to memory pressure and potential out-of-memory errors. It's recommended to cache only the necessary DataFrames and to unpersist them when they are no longer needed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958eb033-677e-49a2-b0f4-cee75cbaccab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### *spark.catalog.clearCache()*\n",
    "\n",
    "*The spark.catalog.clearCache() function in PySpark clears all cached tables and the associated in-memory DataFrame caches. This function is useful for releasing memory resources occupied by cached DataFrames, especially when memory usage needs to be optimized or when cached data becomes outdated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a3eb67-c75c-4912-80b8-1bee370b659a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02dff704-15a5-4c73-bbe5-cf1abb3719e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### When to Use Cache and Persist in Spark\n",
    "\n",
    "##### 1. Repeated Access to Data\n",
    "*Use `cache()` or `persist()` when a DataFrame or RDD is accessed multiple times within the same Spark job. This avoids recalculating the data, saving time and resources.*\n",
    "\n",
    "##### 2. Optimization for Iterative Algorithms\n",
    "For iterative algorithms, like machine learning models (e.g., K-means, PageRank), caching or persisting data prevents redundant computations across iterations.\n",
    "\n",
    "##### 3. Performance Improvement for Expensive Operations\n",
    "When transformations or computations are expensive (e.g., wide transformations like `groupBy`), caching or persisting results can significantly boost performance.\n",
    "\n",
    "##### 4. Interactive and Exploratory Data Analysis\n",
    "During interactive data analysis, caching data can make the experience faster and more responsive, reducing the latency of repeated queries.\n",
    "\n",
    "##### 5. Joining Large Datasets\n",
    "When joining large datasets, caching the smaller dataset (if it fits into memory) can reduce the overhead of shuffling and broadcasting.\n",
    "\n",
    "\n",
    "### When Not to Use Cache and Persist in Spark\n",
    "\n",
    "##### 1. Memory Constraints\n",
    "If your cluster has limited memory, overusing `cache()` and `persist()` can lead to memory pressure, causing out-of-memory errors or forcing Spark to spill data to disk, which is slower.\n",
    "\n",
    "##### 2. Data Used Only Once\n",
    "If the DataFrame or RDD is accessed only once, caching or persisting it is unnecessary and wastes resources.\n",
    "\n",
    "##### 3. Small or Lightweight Transformations\n",
    "For small datasets or lightweight transformations, the overhead of caching may outweigh the benefits, as Spark is optimized for efficient in-memory operations.\n",
    "\n",
    "##### 4. Frequent Data Changes\n",
    "If data is frequently updated or transformed, caching might require frequent invalidation and recomputation, negating performance gains and adding complexity.\n",
    "\n",
    "##### 5. Temporary Data\n",
    "Avoid caching or persisting temporary or intermediate datasets that are only used in subsequent stages. Let Spark manage these automatically within the execution plan.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Cache in Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}